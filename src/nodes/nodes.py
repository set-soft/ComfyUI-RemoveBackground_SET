# Copyright (c) 2025 Salvador E. Tropea
# Copyright (c) 2025 Instituto Nacional de TecnologÃ­a Industrial
# License: GPLv3
# Project: ComfyUI-RemoveBackground_SET
import json
import os
import safetensors.torch
from safetensors import safe_open
from seconohe.downloader import download_file
from seconohe.apply_mask import apply_mask
from seconohe.torch import get_torch_device_options, get_canonical_device
from seconohe.bti import BatchedTensorIterator
# from seconohe.torch import get_pytorch_memory_usage_str
import torch
from comfy_api.latest import io
from comfy import model_management
import folder_paths
from . import (main_logger, MODELS_DIR_KEY, MODELS_DIR, DEFAULT_UPSCALE, CATEGORY_BASIC, CATEGORY_LOAD,
               CATEGORY_ADV)
from .utils.arch import RemBg
from .utils.inspyrenet_config import parse_inspyrenet_config


logger = main_logger
auto_device_type = model_management.get_torch_device().type
models_path_default = folder_paths.get_folder_paths(MODELS_DIR_KEY)[0]
#
# Common options and choices
#
TORCH_DTYPE = {
    "float16": torch.float16,
    "float32": torch.float32,
    "bfloat16": torch.bfloat16,
}
WIDTH_OPT = io.Int.Input("width", default=1024, min=0, max=16384, step=32,
                         tooltip="The width of the pre-processing image, does not affect the final output image size")
HEIGHT_OPT = io.Int.Input("height", default=1024, min=0, max=16384, step=32,
                          tooltip="The height of the pre-processing image, does not affect the final output image size")
UPSCALE_OPT = io.Combo.Input("upscale_method", options=["area", "bicubic", "nearest-exact", "bilinear", "lanczos"],
                             default=DEFAULT_UPSCALE,
                             tooltip="Interpolation method for pre-processing image and post-processing mask")
BLUR_SIZE_OPT = io.Int.Input("blur_size", default=90, min=1, max=255, step=1,
                             tooltip="Diameter for the coarse gaussian blur used for the `Approximate Fast Foreground "
                             "Colour Estimation`")
BLUR_SIZE_TWO_OPT = io.Int.Input("blur_size_two", default=6, min=1, max=255, step=1,
                                 tooltip="Diameter for the fine gaussian blur (see `blur_size`)")
COLOR_OPT = io.String.Input("color", default="#000000", tooltip="Color for fill.\n"
                            "Can be an hexadecimal (#RRGGBB).\n"
                            "Can be comma separated RGB values in [0-255] or [0-1.0] range.\n"
                            "Also `rgb(R,G,B)`, `hsv(H,S,V)` and `hsl(H,S,L) are supported.\n'"
                            "In addition around 140 color names are recognized")
MASK_THRESHOLD_OPT = io.Float.Input("mask_threshold", default=0.0, min=0.0, max=1.0, step=0.001,
                                    tooltip="Most models generates masks that contain a value from 0 to 1, but can be any "
                                    "value in between.\n"
                                    "Matte models can estimate transparency using it. If you need to make the mask 0 or 1, "
                                    "but nothing in between, you can provide a threshold here. Values above it will become 1 "
                                    "and the rest 0.")
DTYPES = ["AUTO", "float32", "float16"]
DTYPE_OPS = io.Combo.Input("dtype", options=DTYPES, default=DTYPES[0], optional=True,
                           tooltip="Data type used for inference. `AUTO` means the same as the model on disk."
                                   "Using a smaller size will save resources, but might degradate the results.")
DTYPE_OUTOPS = io.Combo.Input("out_dtype", options=DTYPES, default=DTYPES[0], optional=True,
                              tooltip="Data type used for the outputs. `AUTO` means the same as the input."
                                      "Using `float16` can help when processing videos.")

DEPTH_OPS = io.Mask.Input("depths", tooltip="For models that starts with a depth map", optional=True)
DIFFDIS_VAE = io.Vae.Input("vae", optional=True, tooltip="SD Turbo VAE for DiffDIS")
POSITIVE = io.Conditioning.Input("positive", optional=True, tooltip="Experimental for DiffDIS")
BATCHED_OPS = io.Int.Input("batch_size", default=1, min=1, max=256, step=1, tooltip="How many images to process at once")
HELP_DIR = "`ComfyUI/models/"+MODELS_DIR+"`"
MODEL_TOOLTIP = "The remove background model from `Load RemBG model by file` or any of the `Load XXXXXX model by name` nodes"
MODEL_OUT_TOOLTIP = "The remove background model ready to be used in a processing node"
MASKS_TOOLTIP = ("The estimated masks, where a higher value means the model estimates it belongs to the foreground "
                 "with more confidence.")
DEPTHS_TOOLTIP = ("The estimated depth map. Either from the `depths` input or computed. Note this applies only to PDFNet.\n"
                  "This is the map generated by `Depth Anything V2`")
EDGES_TOOLTIP = "The estimated edges. This is only generated by the DiffDIS model."
IMAGE_TOOLTIP = "The images with the background removed (transparent) or replaced by the background image"
DEVICE_TOOLTIP = "Device where the model will be run"
IMG_READY_TOOLTIP = ("One or more images to process, they must be normalized to a range that "
                     "is good for the model. Their size must be similar to the size used to train the model.")
IMG_TOOLTIP = "One or more images to process, will be scaled to a size that is good for the model."
SETRemBG = io.Custom("SET_REMBG")
NormParams = io.Custom("NORM_PARAMS")


def dtype_str_to_torch(dtype: str) -> torch.dtype:
    return None if dtype is None or dtype == "AUTO" else TORCH_DTYPE[dtype]


class ModelInfo:
    """
    A simple class to hold model information.
    Attributes are dynamically created from the keys of the input dictionary.
    """
    def __init__(self, data: dict):
        self.no_commercial = False
        # Iterate through the dictionary and set attributes on the instance
        for key, value in data.items():
            setattr(self, key, value)

    def __repr__(self) -> str:
        """Provides a developer-friendly representation of the object."""
        # self.__dict__ holds all the attributes of the instance
        attributes = ', '.join(f"{k}={v!r}" for k, v in self.__dict__.items())
        return f"ModelInfo({attributes})"


class KnownModelsLoader:
    """
    Loads model information from a JSON file and populates a dictionary
    with ModelInfo objects.
    """
    def __init__(self, filename="known_models.json"):
        # The dictionary to store model names -> ModelInfo objects
        self.models = {}

        # --- Safely locate the JSON file ---
        # __file__ is the path to the current script
        # os.path.dirname gets the directory of that script
        # os.path.join creates a platform-independent path to the file
        script_dir = os.path.dirname(__file__)
        file_path = os.path.join(script_dir, filename)

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Could not find {filename} in the script directory: {script_dir}")

        # --- Load and process the file ---
        with open(file_path, 'r') as f:
            filtered_lines = [line for line in f if not line.lstrip().startswith('#')]
            data = json.loads("".join(filtered_lines))

        # Iterate through the top-level dictionary from the JSON
        for model_name, model_data in data.items():
            # Create a ModelInfo instance and store it
            self.models[model_name] = ModelInfo(model_data)


KNOWN_MODELS = KnownModelsLoader().models


def add_inspyrenet_models():
    """ Try to add the transparent-background Python module models """
    # Look for the config and return the models
    models = parse_inspyrenet_config(logger)
    model_t = 'InSPyReNet'
    for m in models:
        ops_name = f"{m.name} (from TB config)"
        name = f"{model_t} {ops_name}"
        if name in KNOWN_MODELS:
            # We already added it
            continue
        KNOWN_MODELS[name] = ModelInfo({'url': m.url, 'file_name': m.ckpt_name, 'train_w': m.base_size[0],
                                        'train_h': m.base_size[1], 'model_t': model_t, 'ops_name': ops_name,
                                        'name': m.name, 'file_t': 'pytorch', 'dtype': 'float32'})


add_inspyrenet_models()


class LoadModel(io.ComfyNode):
    """ Load already downloaded model """
    @classmethod
    def define_schema(cls) -> io.Schema:
        device_options, _ = get_torch_device_options(with_auto=True)
        return io.Schema(
            node_id="LoadRembgByBiRefNetModel_SET",
            display_name="Load RemBG model by file",
            category=CATEGORY_LOAD,
            description=(f"Load background remove model from folder {HELP_DIR}"
                         " or the path of `rembg` configured in the extra YAML file"),
            inputs=[io.Combo.Input("model", options=folder_paths.get_filename_list(MODELS_DIR_KEY),
                                   tooltip=f"The name of the model, the node scans {HELP_DIR}"),
                    io.Combo.Input("device", options=device_options, tooltip=DEVICE_TOOLTIP),
                    DTYPE_OPS, DIFFDIS_VAE, POSITIVE],
            outputs=[SETRemBG.Output(display_name="model", tooltip=MODEL_OUT_TOOLTIP)]
        )

    @classmethod
    def execute(cls, model, device, dtype="auto", vae=None, positive=None) -> io.NodeOutput:
        model_path = model if os.path.isabs(model) else folder_paths.get_full_path(MODELS_DIR_KEY, model)

        # Load the state dict
        logger.debug(f"Loading model weights from {model_path}")
        if model_path.endswith(".safetensors"):
            # Try to get the metadata
            with safe_open(model_path, framework="pt", device="cpu") as f:
                loaded_metadata = f.metadata()
                if loaded_metadata:
                    for key, value in loaded_metadata.items():
                        logger.debug(f"  - {key}: {value}")
            # Load the weights
            state_dict = safetensors.torch.load_file(model_path, device="cpu")
        else:
            state_dict = torch.load(model_path, map_location="cpu")
            if 'model_state_dict' in state_dict:
                # BEN
                state_dict = state_dict['model_state_dict']

        # Check this is valid for a known model
        arch = RemBg(state_dict, logger, model, vae, positive)
        arch.check()
        target_device = get_canonical_device(auto_device_type if device == "AUTO" else device)
        logger.debug(f"Using {target_device} device")
        target_dtype = arch.dtype if dtype == "AUTO" else TORCH_DTYPE[dtype]
        logger.debug(f"Using {target_dtype} data type")

        # Create an instance
        arch.instantiate_model(state_dict, target_device, target_dtype)
        return io.NodeOutput(arch)


class AutoDownloadBiRefNetModel(io.ComfyNode):
    """ Base class for all the auto-downloaders """
    model_type = 'BiRefNet'

    @classmethod
    def define_schema(cls) -> io.Schema:
        device_options, _ = get_torch_device_options(with_auto=True)
        cls.known_models = {v.ops_name: v for k, v in KNOWN_MODELS.items() if v.model_t == cls.model_type}
        inputs = [io.Combo.Input("model_name", options=list(cls.known_models.keys()),
                                 tooltip="The name of the model, from the list of known models of this type"),
                  io.Combo.Input("device", options=device_options, tooltip=DEVICE_TOOLTIP),
                  DTYPE_OPS]
        if cls.model_type == 'DiffDIS':
            inputs.append(DIFFDIS_VAE)
            inputs.append(POSITIVE)
        return io.Schema(
            node_id=cls.UNIQUE_NAME,
            display_name=cls.DISPLAY_NAME,
            category=CATEGORY_LOAD,
            description=cls.DESCRIPTION,
            inputs=inputs,
            outputs=[SETRemBG.Output(display_name="model", tooltip=MODEL_OUT_TOOLTIP),
                     io.Int.Output(display_name="train_w", tooltip="Width of the images used to train this model"),
                     io.Int.Output(display_name="train_h", tooltip="Height of the images used to train this model"),
                     NormParams.Output(display_name="norm_params", tooltip="Normalization parameters for the input images. "
                                       "This is needed only for advanced use when you want "
                                       "to manually pre-process the images. The `Arbitrary Normalize` node from "
                                       "`Image Misc` can use these parameters to apply the correct normalization.")]
        )

    @classmethod
    def fill_description(cls):
        cls.DESCRIPTION = f"Auto download {cls.model_type} model to models/{MODELS_DIR}"
        cls.UNIQUE_NAME = cls.__name__ + "_SET"
        cls.DISPLAY_NAME = f"Load {cls.model_type} model by name"

    @classmethod
    def execute(cls, model_name, device, dtype="float32", vae=None, positive=None) -> io.NodeOutput:
        m = cls.known_models[model_name]
        if m.file_name is None:
            # Use the name in the URL
            fname = os.path.basename(m.url)
        else:
            # Use the extension from the URL
            fname = m.file_name + os.path.splitext(m.url)[1]
        model_full_path = folder_paths.get_full_path(MODELS_DIR_KEY, fname)
        if model_full_path is None:
            download_file(logger, m.url, models_path_default, fname)
        res = LoadModel.execute(fname, device, dtype, vae=vae, positive=positive).result
        arch = res[0]
        # Known training sizes have priority over default architecture sizes
        arch.w = m.train_w
        arch.h = m.train_h
        arch.sub_type = m.name
        if m.no_commercial:
            logger.warning(f"`{arch.get_name()}` model isn't for commercial use!")
        return io.NodeOutput(arch, m.train_w, m.train_h, {"mean": arch.img_mean, "std": arch.img_std})


# BiRefNet is de default
AutoDownloadBiRefNetModel.fill_description()


class AutoDownloadBENModel(AutoDownloadBiRefNetModel):
    model_type = 'MVANet'


# People knows about BEN
AutoDownloadBENModel.model_type = 'MVANet/BEN'
AutoDownloadBENModel.fill_description()
AutoDownloadBENModel.model_type = 'MVANet'


class AutoDownloadInSPyReNetModel(AutoDownloadBiRefNetModel):
    model_type = 'InSPyReNet'


AutoDownloadInSPyReNetModel.fill_description()


class AutoDownloadU2NetModel(AutoDownloadBiRefNetModel):
    model_type = 'U-2-Net'


AutoDownloadU2NetModel.fill_description()


class AutoDownloadISNetModel(AutoDownloadBiRefNetModel):
    model_type = 'IS-Net'


AutoDownloadISNetModel.fill_description()


class AutoDownloadMODNetModel(AutoDownloadBiRefNetModel):
    model_type = 'MODNet'


AutoDownloadMODNetModel.fill_description()


class AutoDownloadPDFNetModel(AutoDownloadBiRefNetModel):
    model_type = 'PDFNet'


AutoDownloadPDFNetModel.fill_description()


class AutoDownloadDiffDISModel(AutoDownloadBiRefNetModel):
    model_type = 'DiffDIS'


AutoDownloadDiffDISModel.fill_description()


class GetMaskLow(io.ComfyNode):
    @classmethod
    def define_schema(cls) -> io.Schema:
        return io.Schema(
            node_id="GetMaskLowByBiRefNet_SET",
            display_name="Get background mask low level",
            category=CATEGORY_ADV,
            description=("Computes the foreground mask. No pre or post processing is applied, you must do it outside the "
                         "node.\nSee the `Get background mask` node"),
            inputs=[SETRemBG.Input("model", tooltip=MODEL_TOOLTIP),
                    io.Image.Input("images", tooltip=IMG_READY_TOOLTIP),
                    BATCHED_OPS,
                    DEPTH_OPS,
                    DTYPE_OUTOPS],
            outputs=[io.Mask.Output(display_name="masks", tooltip=MASKS_TOOLTIP),
                     io.Mask.Output(display_name="depths", tooltip=DEPTHS_TOOLTIP),
                     io.Mask.Output(display_name="edges", tooltip=EDGES_TOOLTIP)]
        )

    @classmethod
    def execute(cls, model, images, batch_size, depths=None, out_dtype=None) -> io.NodeOutput:
        return io.NodeOutput(*model.run_inference(images, depths, batch_size, keep_depths=True, keep_edges=True,
                                                  keep_masks=True, out_dtype=dtype_str_to_torch(out_dtype))[1:])


class GetMask(io.ComfyNode):
    @classmethod
    def define_schema(cls) -> io.Schema:
        return io.Schema(
            node_id="GetMaskByBiRefNet_SET",
            display_name="Get background mask",
            category=CATEGORY_BASIC,
            description=("Computes the foreground mask. It normalizes the input images, scales them to the model size, "
                         "computes the masks and then scales the masks to the image size.\n"
                         "No background removal/replacement is done.\n"
                         "See the `Remove background (full)`."),
            inputs=[SETRemBG.Input("model", tooltip=MODEL_TOOLTIP),
                    io.Image.Input("images", tooltip=IMG_TOOLTIP),
                    WIDTH_OPT,
                    HEIGHT_OPT,
                    UPSCALE_OPT,
                    MASK_THRESHOLD_OPT,
                    BATCHED_OPS,
                    DEPTH_OPS,
                    DTYPE_OUTOPS],
            outputs=[io.Mask.Output(display_name="masks", tooltip=MASKS_TOOLTIP),
                     io.Mask.Output(display_name="depths", tooltip=DEPTHS_TOOLTIP),
                     io.Mask.Output(display_name="edges", tooltip=EDGES_TOOLTIP)]
        )

    @classmethod
    def execute(cls, model, images, width=1024, height=1024, upscale_method=DEFAULT_UPSCALE, mask_threshold=0.000,
                batch_size=1, depths=None, out_dtype=None) -> io.NodeOutput:
        return io.NodeOutput(*model.run_inference(images, depths, batch_size,
                                                  model_w=width, model_h=height, scale_method=upscale_method,
                                                  preproc_img=True, mask_threshold=mask_threshold,
                                                  keep_depths=True, keep_edges=True, keep_masks=True,
                                                  out_dtype=dtype_str_to_torch(out_dtype))[1:])


class ImageComposer(object):
    def __init__(self, blur_size, blur_size_two, fill_color, color, background, batch_size, target_device, target_dtype):
        self.blur_size = blur_size
        self.blur_size_two = blur_size_two
        self.fill_color = fill_color
        self.color = color
        if background is not None:
            self.background_iterator = BatchedTensorIterator(tensor=background, sub_batch_size=batch_size,
                                                             device=target_device, dtype=target_dtype)
        else:
            self.background_iterator = None
        self.background = background

    def compose(self, images_bchw, masks_bchw, batch_range):
        background = (None if self.background_iterator is None else
                      self.background_iterator.get_aux_batch(self.background, batch_range))
        out_images = apply_mask(logger, images_bchw.movedim(1, -1), masks=masks_bchw.squeeze(1),
                                device=model_management.get_torch_device(),
                                blur_size=self.blur_size, blur_size_two=self.blur_size_two, fill_color=self.fill_color,
                                color=self.color, batched=True, background=background)
        return out_images.movedim(-1, 1)


class Advanced(io.ComfyNode):
    @classmethod
    def define_schema(cls) -> io.Schema:
        return io.Schema(
            node_id="RembgByBiRefNetAdvanced_SET",
            display_name="Remove background (full)",
            category=CATEGORY_ADV,
            description=("Removes or replaces the background from the input image.\n"
                         "Gives more options and also generates masks and other stuff."),
            inputs=[SETRemBG.Input("model", tooltip=MODEL_TOOLTIP),
                    io.Image.Input("images", tooltip=IMG_TOOLTIP),
                    WIDTH_OPT,
                    HEIGHT_OPT,
                    UPSCALE_OPT,
                    BLUR_SIZE_OPT,
                    BLUR_SIZE_TWO_OPT,
                    io.Boolean.Input("fill_color", default=False,
                                     tooltip="When enabled and no background is provided we "
                                     "fill the background using a color."),
                    COLOR_OPT,
                    MASK_THRESHOLD_OPT,
                    BATCHED_OPS,
                    DEPTH_OPS,
                    io.Image.Input("background", tooltip="Image to use as background", optional=True),
                    DTYPE_OUTOPS],
            outputs=[io.Image.Output(display_name="images", tooltip=IMAGE_TOOLTIP),
                     io.Mask.Output(display_name="masks", tooltip=MASKS_TOOLTIP),
                     io.Mask.Output(display_name="depths", tooltip=DEPTHS_TOOLTIP),
                     io.Mask.Output(display_name="edges", tooltip=EDGES_TOOLTIP)]
        )

    @classmethod
    def execute(cls, model, images, upscale_method=DEFAULT_UPSCALE, width=1024, height=1024, blur_size=91, blur_size_two=7,
                fill_color=False, color=None, mask_threshold=0.000, batch_size=True, depths=None, background=None,
                keep_misc=True, out_dtype=None) -> io.NodeOutput:

        composer = ImageComposer(blur_size, blur_size_two, fill_color, color, background, batch_size, model.target_device,
                                 model.target_dtype)
        return io.NodeOutput(*model.run_inference(images, depths, batch_size, model_w=width, model_h=height,
                                                  scale_method=upscale_method, preproc_img=True,
                                                  mask_threshold=mask_threshold,
                                                  image_compose=composer,
                                                  keep_depths=keep_misc, keep_edges=keep_misc, keep_masks=keep_misc,
                                                  out_dtype=dtype_str_to_torch(out_dtype)))


class RemBGSimple(io.ComfyNode):
    @classmethod
    def define_schema(cls) -> io.Schema:
        return io.Schema(
            node_id="RembgByBiRefNet_SET",
            display_name="Remove background",
            category=CATEGORY_BASIC,
            description="Removes or replaces the background from the input image.",
            inputs=[SETRemBG.Input("model", tooltip=MODEL_TOOLTIP),
                    io.Image.Input("images", tooltip=IMG_TOOLTIP),
                    BATCHED_OPS,
                    DEPTH_OPS,
                    io.Image.Input("background", tooltip="Image to use as background", optional=True),
                    DTYPE_OUTOPS],
            outputs=[io.Image.Output(display_name="images", tooltip=IMAGE_TOOLTIP)]
        )

    @classmethod
    def execute(cls, model, images, batch_size, depths=None, background=None, out_dtype=None) -> io.NodeOutput:
        w = model.w
        h = model.h
        logger.debug(f"Using size {w}x{h}")
        return io.NodeOutput(Advanced.execute(model, images, width=w, height=h, batch_size=batch_size, depths=depths,
                                              background=background, keep_misc=False, out_dtype=out_dtype)[0])
